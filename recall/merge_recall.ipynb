{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 合并多路召回的结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from typing import Dict, Tuple, List, Set\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "import copy\n",
    "import faiss\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"7\"\n",
    "import tensorflow as tf\n",
    "logger = tf.get_logger()\n",
    "logger.setLevel('ERROR')  # 设置日志级别为ERROR\n",
    "import polars as pl\n",
    "import pickle\n",
    "from utils.exposure_data import get_all_expose_df, get_user_item_time\n",
    "from data.dataset import NewsDataset\n",
    "from data.preprocessing import build_feature_columns\n",
    "from utils.config import load_configs\n",
    "# 设置文件目录\n",
    "offline = False\n",
    "mode = \"offline\" if offline else \"online\"\n",
    "data_path = f\"/data3/zxh/news_rec/{mode}_data\"\n",
    "public_path = \"/data3/zxh/news_rec/public_data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. ItemCF召回"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def itemcf_recall(user_id, user_item_time_dict, itemcf_inverted_index, sim_item_topk=10, recall_item_num=50):\n",
    "    \"\"\"\n",
    "    基于物品协同过滤的召回通道。\n",
    "    :param user_id: 用户ID\n",
    "    :param user_item_time_dict: 用户历史点击记录 {user_id: [(item_id, timestamp), ...]}\n",
    "    :param itemcf_inverted_index: 物品倒排索引表 {item_id: [(similar_item_id, similarity_score), ...]}\n",
    "    :param sim_item_topk: 选择每个物品最相似的前K个物品\n",
    "    :param recall_item_num: 召回的物品数量\n",
    "    :param last_n: 根据每个用户的LastN进行召回\n",
    "    :return: 召回的物品列表 {item1, item2, ...}\n",
    "    \"\"\"\n",
    "    user_hist_items = list(dict.fromkeys(click_article_id for click_article_id, _ in user_item_time_dict.get(user_id, [])))\n",
    "    item_rank = defaultdict(float)\n",
    "    \n",
    "    # 遍历用户点击过的物品\n",
    "    for item in user_hist_items:\n",
    "        # 获取与当前物品最相似的topK物品\n",
    "        for similar_item, similarity in itemcf_inverted_index[item][:sim_item_topk]:\n",
    "            # 过滤掉用户已经点击过的物品\n",
    "            if similar_item in user_hist_items:\n",
    "                continue\n",
    "            \n",
    "            # 计算用户的兴趣得分\n",
    "            item_rank[similar_item] += similarity\n",
    "\n",
    "    return [item for item, like_socre in sorted(item_rank.items(), key=lambda x: x[1], reverse=True)[:recall_item_num]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"/data3/zxh/news_rec/temp_results/itemcf_top_100_inverted_index_{mode}.pkl\", \"rb\") as f:\n",
    "    itemcf_inverted_index = pickle.load(f)\n",
    "train_df, _ = get_all_expose_df(offline=offline) # 获取训练数据集\n",
    "user_item_time_dict = get_user_item_time(train_df) # 获取用户的 last-N 点击"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Swing召回"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"/data3/zxh/news_rec/temp_results/swing_top_100_inverted_index_{mode}.pkl\", \"rb\") as f:\n",
    "    swing_inverted_index = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def swing_recall(user_id, user_item_time_dict, swing_inverted_index, sim_item_topk=10, recall_item_num=50):\n",
    "    \"\"\"\n",
    "    基于 Swing 相似度的召回通道。\n",
    "    \n",
    "    :param user_id: 用户ID\n",
    "    :param user_item_time_dict: 用户历史点击记录 {user_id: [(item_id, timestamp), ...]}\n",
    "    :param swing_inverted_index: Swing 物品倒排索引表 {item_id: [(similar_item_id, similarity_score), ...]}\n",
    "    :param sim_item_topk: 选择每个物品最相似的前K个物品\n",
    "    :param recall_item_num: 召回的物品数量\n",
    "    :param last_n: 使用最近的 N 条点击记录进行召回\n",
    "    :return: 召回的物品集合 {item1, item2, ...}\n",
    "    \"\"\"\n",
    "    # 去重+保序\n",
    "    user_hist_items = list(dict.fromkeys(click_article_id for click_article_id, _ in user_item_time_dict.get(user_id, [])))\n",
    "    item_rank = defaultdict(float)\n",
    "\n",
    "    # 遍历最近的点击物品\n",
    "    for item in user_hist_items:\n",
    "        # 获取 Swing 倒排索引中与该 item 最相似的 sim_item_topk 个物品\n",
    "        for similar_item, similarity in swing_inverted_index.get(item, [])[:sim_item_topk]:\n",
    "            if similar_item in user_hist_items:\n",
    "                continue\n",
    "            item_rank[similar_item] += similarity\n",
    "\n",
    "    # 返回按兴趣得分排序的 topN 物品\n",
    "    return [item for item, score in sorted(item_rank.items(), key=lambda x: x[1], reverse=True)[:recall_item_num]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. 双塔召回"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.1 准备模型和数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-07 17:10:10.276767: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-04-07 17:10:10.680369: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 685 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:b2:00.0, compute capability: 8.6\n",
      "2025-04-07 17:10:11.330550: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "2025-04-07 17:10:11.389953: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 230400288 exceeds 10% of free system memory.\n",
      "2025-04-07 17:10:13.222133: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 216000288 exceeds 10% of free system memory.\n"
     ]
    }
   ],
   "source": [
    "# 加载用户塔模型\n",
    "user_embedding_model = tf.keras.models.load_model(\"/data3/zxh/news_rec/temp_results/dssm_user/003_online/\")\n",
    "\n",
    "# 加载物品塔模型\n",
    "item_embedding_model = tf.keras.models.load_model(\"/data3/zxh/news_rec/temp_results/dssm_item/003_online/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载配置\n",
    "base_config = load_configs()\n",
    "# 加载特征列\n",
    "feature_columns, feature_groups = build_feature_columns(base_config['feature'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_config, user_config = copy.deepcopy(base_config), copy.deepcopy(base_config)\n",
    "item_config[\"feature\"][\"csv_schema\"] = [\"article_id\",\"keywords\",\"image_count\",\"category_level1\",\"category_level2\",\n",
    "                                        \"docid_history_count\",\"docid_expose_count\",\"docid_ctr\",\"docid_history_duration_mean\",\n",
    "                                        \"category1_ctr\",\"category1_popularity\",\"category1_history_duration_mean\",\n",
    "                                        \"category2_ctr\",\"category2_popularity\",\"category2_history_duration_mean\"]\n",
    "user_config[\"feature\"][\"csv_schema\"] = [\"user_id\", \"network_env\", \"refresh_count\",\"device_name\",\"os\",\"province\",\n",
    "                                        \"city\",\"age\",\"gender\",\"userid_history_duration_mean\",\"userid_history_count\",\n",
    "                                        \"userid_expose_count\",\"userid_ctr\", \"expose_hour\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (12_256_166, 4)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>user_id</th><th>article_id</th><th>expose_hour</th><th>is_clicked</th></tr><tr><td>i64</td><td>i64</td><td>i64</td><td>i64</td></tr></thead><tbody><tr><td>2413368274</td><td>465760067</td><td>4</td><td>0</td></tr><tr><td>2231512322</td><td>466772262</td><td>13</td><td>0</td></tr><tr><td>2240894400</td><td>466655257</td><td>10</td><td>0</td></tr><tr><td>1486216632</td><td>466651314</td><td>12</td><td>0</td></tr><tr><td>2439239452</td><td>466398247</td><td>0</td><td>0</td></tr><tr><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td></tr><tr><td>1406769714</td><td>466077472</td><td>13</td><td>0</td></tr><tr><td>2430514764</td><td>466449125</td><td>4</td><td>0</td></tr><tr><td>2092924226</td><td>466636411</td><td>10</td><td>0</td></tr><tr><td>1468517980</td><td>466336113</td><td>2</td><td>0</td></tr><tr><td>2440918550</td><td>466752487</td><td>13</td><td>0</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (12_256_166, 4)\n",
       "┌────────────┬────────────┬─────────────┬────────────┐\n",
       "│ user_id    ┆ article_id ┆ expose_hour ┆ is_clicked │\n",
       "│ ---        ┆ ---        ┆ ---         ┆ ---        │\n",
       "│ i64        ┆ i64        ┆ i64         ┆ i64        │\n",
       "╞════════════╪════════════╪═════════════╪════════════╡\n",
       "│ 2413368274 ┆ 465760067  ┆ 4           ┆ 0          │\n",
       "│ 2231512322 ┆ 466772262  ┆ 13          ┆ 0          │\n",
       "│ 2240894400 ┆ 466655257  ┆ 10          ┆ 0          │\n",
       "│ 1486216632 ┆ 466651314  ┆ 12          ┆ 0          │\n",
       "│ 2439239452 ┆ 466398247  ┆ 0           ┆ 0          │\n",
       "│ …          ┆ …          ┆ …           ┆ …          │\n",
       "│ 1406769714 ┆ 466077472  ┆ 13          ┆ 0          │\n",
       "│ 2430514764 ┆ 466449125  ┆ 4           ┆ 0          │\n",
       "│ 2092924226 ┆ 466636411  ┆ 10          ┆ 0          │\n",
       "│ 1468517980 ┆ 466336113  ┆ 2           ┆ 0          │\n",
       "│ 2440918550 ┆ 466752487  ┆ 13          ┆ 0          │\n",
       "└────────────┴────────────┴─────────────┴────────────┘"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = pl.read_csv(\"/data3/zxh/news_rec/recall_csv_data/test_data/test_data.csv\", separator=\"\\t\")\n",
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-07 17:10:28.174088: W tensorflow/core/common_runtime/bfc_allocator.cc:457] Allocator (GPU_0_bfc) ran out of memory trying to allocate 8B (rounded to 256)requested by op StridedSlice\n",
      "If the cause is memory fragmentation maybe the environment variable 'TF_GPU_ALLOCATOR=cuda_malloc_async' will improve the situation. \n",
      "Current allocation summary follows.\n",
      "Current allocation summary follows.\n",
      "2025-04-07 17:10:28.174163: I tensorflow/core/common_runtime/bfc_allocator.cc:1004] BFCAllocator dump for GPU_0_bfc\n",
      "2025-04-07 17:10:28.174189: I tensorflow/core/common_runtime/bfc_allocator.cc:1011] Bin (256): \tTotal Chunks: 8, Chunks in use: 8. 2.0KiB allocated for chunks. 2.0KiB in use in bin. 484B client-requested in use in bin.\n",
      "2025-04-07 17:10:28.174203: I tensorflow/core/common_runtime/bfc_allocator.cc:1011] Bin (512): \tTotal Chunks: 2, Chunks in use: 2. 1.0KiB allocated for chunks. 1.0KiB in use in bin. 640B client-requested in use in bin.\n",
      "2025-04-07 17:10:28.174217: I tensorflow/core/common_runtime/bfc_allocator.cc:1011] Bin (1024): \tTotal Chunks: 6, Chunks in use: 6. 6.8KiB allocated for chunks. 6.8KiB in use in bin. 6.0KiB client-requested in use in bin.\n",
      "2025-04-07 17:10:28.174229: I tensorflow/core/common_runtime/bfc_allocator.cc:1011] Bin (2048): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2025-04-07 17:10:28.174242: I tensorflow/core/common_runtime/bfc_allocator.cc:1011] Bin (4096): \tTotal Chunks: 1, Chunks in use: 1. 7.2KiB allocated for chunks. 7.2KiB in use in bin. 7.1KiB client-requested in use in bin.\n",
      "2025-04-07 17:10:28.174255: I tensorflow/core/common_runtime/bfc_allocator.cc:1011] Bin (8192): \tTotal Chunks: 1, Chunks in use: 1. 9.5KiB allocated for chunks. 9.5KiB in use in bin. 9.4KiB client-requested in use in bin.\n",
      "2025-04-07 17:10:28.174270: I tensorflow/core/common_runtime/bfc_allocator.cc:1011] Bin (16384): \tTotal Chunks: 1, Chunks in use: 1. 19.0KiB allocated for chunks. 19.0KiB in use in bin. 18.8KiB client-requested in use in bin.\n",
      "2025-04-07 17:10:28.174284: I tensorflow/core/common_runtime/bfc_allocator.cc:1011] Bin (32768): \tTotal Chunks: 2, Chunks in use: 2. 125.0KiB allocated for chunks. 125.0KiB in use in bin. 125.0KiB client-requested in use in bin.\n",
      "2025-04-07 17:10:28.174299: I tensorflow/core/common_runtime/bfc_allocator.cc:1011] Bin (65536): \tTotal Chunks: 2, Chunks in use: 2. 214.0KiB allocated for chunks. 214.0KiB in use in bin. 213.8KiB client-requested in use in bin.\n",
      "2025-04-07 17:10:28.174313: I tensorflow/core/common_runtime/bfc_allocator.cc:1011] Bin (131072): \tTotal Chunks: 3, Chunks in use: 3. 652.5KiB allocated for chunks. 652.5KiB in use in bin. 652.5KiB client-requested in use in bin.\n",
      "2025-04-07 17:10:28.174324: I tensorflow/core/common_runtime/bfc_allocator.cc:1011] Bin (262144): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2025-04-07 17:10:28.174334: I tensorflow/core/common_runtime/bfc_allocator.cc:1011] Bin (524288): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2025-04-07 17:10:28.174357: I tensorflow/core/common_runtime/bfc_allocator.cc:1011] Bin (1048576): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2025-04-07 17:10:28.174362: I tensorflow/core/common_runtime/bfc_allocator.cc:1011] Bin (2097152): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2025-04-07 17:10:28.174367: I tensorflow/core/common_runtime/bfc_allocator.cc:1011] Bin (4194304): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2025-04-07 17:10:28.174371: I tensorflow/core/common_runtime/bfc_allocator.cc:1011] Bin (8388608): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2025-04-07 17:10:28.174375: I tensorflow/core/common_runtime/bfc_allocator.cc:1011] Bin (16777216): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2025-04-07 17:10:28.174380: I tensorflow/core/common_runtime/bfc_allocator.cc:1011] Bin (33554432): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2025-04-07 17:10:28.174386: I tensorflow/core/common_runtime/bfc_allocator.cc:1011] Bin (67108864): \tTotal Chunks: 3, Chunks in use: 3. 259.20MiB allocated for chunks. 259.20MiB in use in bin. 240.08MiB client-requested in use in bin.\n",
      "2025-04-07 17:10:28.174393: I tensorflow/core/common_runtime/bfc_allocator.cc:1011] Bin (134217728): \tTotal Chunks: 2, Chunks in use: 2. 425.72MiB allocated for chunks. 425.72MiB in use in bin. 425.72MiB client-requested in use in bin.\n",
      "2025-04-07 17:10:28.174409: I tensorflow/core/common_runtime/bfc_allocator.cc:1011] Bin (268435456): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2025-04-07 17:10:28.174414: I tensorflow/core/common_runtime/bfc_allocator.cc:1027] Bin for 256B was 256B, Chunk State: \n",
      "2025-04-07 17:10:28.174418: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Next region of size 719257600\n",
      "2025-04-07 17:10:28.174426: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 7379ec000000 of size 256 next 1\n",
      "2025-04-07 17:10:28.174430: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 7379ec000100 of size 1280 next 2\n",
      "2025-04-07 17:10:28.174434: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 7379ec000600 of size 230400512 next 3\n",
      "2025-04-07 17:10:28.174439: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 7379f9bba800 of size 128256 next 4\n",
      "2025-04-07 17:10:28.174443: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 7379f9bd9d00 of size 256 next 5\n",
      "2025-04-07 17:10:28.174447: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 7379f9bd9e00 of size 9728 next 6\n",
      "2025-04-07 17:10:28.174451: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 7379f9bdc400 of size 19456 next 7\n",
      "2025-04-07 17:10:28.174455: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 7379f9be1000 of size 256 next 8\n",
      "2025-04-07 17:10:28.174458: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 7379f9be1100 of size 256 next 9\n",
      "2025-04-07 17:10:28.174462: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 7379f9be1200 of size 256 next 10\n",
      "2025-04-07 17:10:28.174466: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 7379f9be1300 of size 256 next 11\n",
      "2025-04-07 17:10:28.174470: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 7379f9be1400 of size 1280 next 12\n",
      "2025-04-07 17:10:28.174474: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 7379f9be1900 of size 256000 next 13\n",
      "2025-04-07 17:10:28.174477: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 7379f9c20100 of size 1024 next 14\n",
      "2025-04-07 17:10:28.174486: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 7379f9c20500 of size 64000 next 15\n",
      "2025-04-07 17:10:28.174490: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 7379f9c2ff00 of size 512 next 16\n",
      "2025-04-07 17:10:28.174494: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 7379f9c30100 of size 90880 next 17\n",
      "2025-04-07 17:10:28.174498: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 7379f9c46400 of size 89600256 next 18\n",
      "2025-04-07 17:10:28.174501: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 7379ff1b9500 of size 256 next 19\n",
      "2025-04-07 17:10:28.174505: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 7379ff1b9600 of size 1024 next 20\n",
      "2025-04-07 17:10:28.174509: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 7379ff1b9a00 of size 7424 next 21\n",
      "2025-04-07 17:10:28.174513: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 7379ff1bb700 of size 216000512 next 22\n",
      "2025-04-07 17:10:28.174516: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 737a0bfb9f00 of size 512 next 23\n",
      "2025-04-07 17:10:28.174520: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 737a0bfba100 of size 156160 next 24\n",
      "2025-04-07 17:10:28.174524: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 737a0bfe0300 of size 1280 next 25\n",
      "2025-04-07 17:10:28.174528: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 737a0bfe0800 of size 256000 next 26\n",
      "2025-04-07 17:10:28.174531: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 737a0c01f000 of size 1024 next 27\n",
      "2025-04-07 17:10:28.174536: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 737a0c01f400 of size 64000 next 28\n",
      "2025-04-07 17:10:28.174540: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 737a0c02ee00 of size 81073920 next 29\n",
      "2025-04-07 17:10:28.174544: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 737a10d80500 of size 256 next 30\n",
      "2025-04-07 17:10:28.174550: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 737a10d80600 of size 101120512 next 18446744073709551615\n",
      "2025-04-07 17:10:28.174554: I tensorflow/core/common_runtime/bfc_allocator.cc:1065]      Summary of in-use Chunks by size: \n",
      "2025-04-07 17:10:28.174560: I tensorflow/core/common_runtime/bfc_allocator.cc:1068] 8 Chunks of size 256 totalling 2.0KiB\n",
      "2025-04-07 17:10:28.174564: I tensorflow/core/common_runtime/bfc_allocator.cc:1068] 2 Chunks of size 512 totalling 1.0KiB\n",
      "2025-04-07 17:10:28.174568: I tensorflow/core/common_runtime/bfc_allocator.cc:1068] 3 Chunks of size 1024 totalling 3.0KiB\n",
      "2025-04-07 17:10:28.174572: I tensorflow/core/common_runtime/bfc_allocator.cc:1068] 3 Chunks of size 1280 totalling 3.8KiB\n",
      "2025-04-07 17:10:28.174576: I tensorflow/core/common_runtime/bfc_allocator.cc:1068] 1 Chunks of size 7424 totalling 7.2KiB\n",
      "2025-04-07 17:10:28.174580: I tensorflow/core/common_runtime/bfc_allocator.cc:1068] 1 Chunks of size 9728 totalling 9.5KiB\n",
      "2025-04-07 17:10:28.174585: I tensorflow/core/common_runtime/bfc_allocator.cc:1068] 1 Chunks of size 19456 totalling 19.0KiB\n",
      "2025-04-07 17:10:28.174589: I tensorflow/core/common_runtime/bfc_allocator.cc:1068] 2 Chunks of size 64000 totalling 125.0KiB\n",
      "2025-04-07 17:10:28.174594: I tensorflow/core/common_runtime/bfc_allocator.cc:1068] 1 Chunks of size 90880 totalling 88.8KiB\n",
      "2025-04-07 17:10:28.174598: I tensorflow/core/common_runtime/bfc_allocator.cc:1068] 1 Chunks of size 128256 totalling 125.2KiB\n",
      "2025-04-07 17:10:28.174602: I tensorflow/core/common_runtime/bfc_allocator.cc:1068] 1 Chunks of size 156160 totalling 152.5KiB\n",
      "2025-04-07 17:10:28.174607: I tensorflow/core/common_runtime/bfc_allocator.cc:1068] 2 Chunks of size 256000 totalling 500.0KiB\n",
      "2025-04-07 17:10:28.174612: I tensorflow/core/common_runtime/bfc_allocator.cc:1068] 1 Chunks of size 81073920 totalling 77.32MiB\n",
      "2025-04-07 17:10:28.174616: I tensorflow/core/common_runtime/bfc_allocator.cc:1068] 1 Chunks of size 89600256 totalling 85.45MiB\n",
      "2025-04-07 17:10:28.174621: I tensorflow/core/common_runtime/bfc_allocator.cc:1068] 1 Chunks of size 101120512 totalling 96.44MiB\n",
      "2025-04-07 17:10:28.174625: I tensorflow/core/common_runtime/bfc_allocator.cc:1068] 1 Chunks of size 216000512 totalling 205.99MiB\n",
      "2025-04-07 17:10:28.174630: I tensorflow/core/common_runtime/bfc_allocator.cc:1068] 1 Chunks of size 230400512 totalling 219.73MiB\n",
      "2025-04-07 17:10:28.174634: I tensorflow/core/common_runtime/bfc_allocator.cc:1072] Sum Total of in-use chunks: 685.94MiB\n",
      "2025-04-07 17:10:28.174638: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] total_region_allocated_bytes_: 719257600 memory_limit_: 719257600 available bytes: 0 curr_region_allocation_bytes_: 1438515200\n",
      "2025-04-07 17:10:28.174647: I tensorflow/core/common_runtime/bfc_allocator.cc:1080] Stats: \n",
      "Limit:                       719257600\n",
      "InUse:                       719257600\n",
      "MaxInUse:                    719257600\n",
      "NumAllocs:                          31\n",
      "MaxAllocSize:                230400512\n",
      "Reserved:                            0\n",
      "PeakReserved:                        0\n",
      "LargestFreeBlock:                    0\n",
      "\n",
      "2025-04-07 17:10:28.174654: W tensorflow/core/common_runtime/bfc_allocator.cc:468] **************************************************************************************************xx\n"
     ]
    },
    {
     "ename": "InternalError",
     "evalue": "Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run StridedSlice: Dst tensor is not initialized. [Op:StridedSlice] name: strided_slice/",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 6\u001b[0m\n\u001b[1;32m      2\u001b[0m item_dataset_creator \u001b[38;5;241m=\u001b[39m NewsDataset(item_config, feature_groups[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mitem\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m      3\u001b[0m user_dataset_creator \u001b[38;5;241m=\u001b[39m NewsDataset(user_config, feature_groups[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m      5\u001b[0m test_dataset \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m----> 6\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mitem\u001b[39m\u001b[38;5;124m\"\u001b[39m : \u001b[43mitem_dataset_creator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/data3/zxh/news_rec/recall_csv_data/test_data/test_item_features.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mmap(\u001b[38;5;28;01mlambda\u001b[39;00m x, y: x),\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m : user_dataset_creator\u001b[38;5;241m.\u001b[39mcreate_dataset(data_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/data3/zxh/news_rec/recall_csv_data/test_data/test_user_features.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mmap(\u001b[38;5;28;01mlambda\u001b[39;00m x, y: x),\n\u001b[1;32m      8\u001b[0m }\n",
      "File \u001b[0;32m~/news_rec/recall/data/dataset.py:187\u001b[0m, in \u001b[0;36mNewsDataset.create_dataset\u001b[0;34m(self, data_path, shuffle)\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"创建数据管道\"\"\"\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;66;03m# 构建数据集\u001b[39;00m\n\u001b[0;32m--> 187\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlist_files\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    188\u001b[0m dataset \u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39minterleave(\n\u001b[1;32m    189\u001b[0m     \u001b[38;5;28;01mlambda\u001b[39;00m filepath: tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mTextLineDataset(filepath)\u001b[38;5;241m.\u001b[39mskip(\u001b[38;5;241m1\u001b[39m),\n\u001b[1;32m    190\u001b[0m     num_parallel_calls\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mAUTOTUNE,\n\u001b[1;32m    191\u001b[0m     deterministic\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m shuffle \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    192\u001b[0m )\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m shuffle:\n",
      "File \u001b[0;32m/data2/Anaconda3/envs/tensorflow/lib/python3.9/site-packages/tensorflow/python/data/ops/dataset_ops.py:1239\u001b[0m, in \u001b[0;36mDatasetV2.list_files\u001b[0;34m(file_pattern, shuffle, seed)\u001b[0m\n\u001b[1;32m   1234\u001b[0m dataset \u001b[38;5;241m=\u001b[39m Dataset\u001b[38;5;241m.\u001b[39mfrom_tensor_slices(matching_files)\n\u001b[1;32m   1235\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m shuffle:\n\u001b[1;32m   1236\u001b[0m   \u001b[38;5;66;03m# NOTE(mrry): The shuffle buffer size must be greater than zero, but the\u001b[39;00m\n\u001b[1;32m   1237\u001b[0m   \u001b[38;5;66;03m# list of files might be empty.\u001b[39;00m\n\u001b[1;32m   1238\u001b[0m   buffer_size \u001b[38;5;241m=\u001b[39m math_ops\u001b[38;5;241m.\u001b[39mmaximum(\n\u001b[0;32m-> 1239\u001b[0m       \u001b[43marray_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmatching_files\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mint64\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m   1240\u001b[0m   dataset \u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39mshuffle(buffer_size, seed\u001b[38;5;241m=\u001b[39mseed)\n\u001b[1;32m   1241\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m dataset\n",
      "File \u001b[0;32m/data2/Anaconda3/envs/tensorflow/lib/python3.9/site-packages/tensorflow/python/util/dispatch.py:206\u001b[0m, in \u001b[0;36madd_dispatch_support.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[39;00m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 206\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtarget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mTypeError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m):\n\u001b[1;32m    208\u001b[0m   \u001b[38;5;66;03m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[39;00m\n\u001b[1;32m    209\u001b[0m   \u001b[38;5;66;03m# TypeError, when given unexpected types.  So we need to catch both.\u001b[39;00m\n\u001b[1;32m    210\u001b[0m   result \u001b[38;5;241m=\u001b[39m dispatch(wrapper, args, kwargs)\n",
      "File \u001b[0;32m/data2/Anaconda3/envs/tensorflow/lib/python3.9/site-packages/tensorflow/python/ops/array_ops.py:1041\u001b[0m, in \u001b[0;36m_slice_helper\u001b[0;34m(tensor, slice_spec, var)\u001b[0m\n\u001b[1;32m   1039\u001b[0m   var_empty \u001b[38;5;241m=\u001b[39m constant([], dtype\u001b[38;5;241m=\u001b[39mdtypes\u001b[38;5;241m.\u001b[39mint32)\n\u001b[1;32m   1040\u001b[0m   packed_begin \u001b[38;5;241m=\u001b[39m packed_end \u001b[38;5;241m=\u001b[39m packed_strides \u001b[38;5;241m=\u001b[39m var_empty\n\u001b[0;32m-> 1041\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mstrided_slice\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1042\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1043\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpacked_begin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1044\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpacked_end\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1045\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpacked_strides\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1046\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbegin_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbegin_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1047\u001b[0m \u001b[43m    \u001b[49m\u001b[43mend_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mend_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1048\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshrink_axis_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshrink_axis_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1049\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnew_axis_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_axis_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1050\u001b[0m \u001b[43m    \u001b[49m\u001b[43mellipsis_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mellipsis_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1051\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1052\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data2/Anaconda3/envs/tensorflow/lib/python3.9/site-packages/tensorflow/python/util/dispatch.py:206\u001b[0m, in \u001b[0;36madd_dispatch_support.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[39;00m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 206\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtarget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mTypeError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m):\n\u001b[1;32m    208\u001b[0m   \u001b[38;5;66;03m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[39;00m\n\u001b[1;32m    209\u001b[0m   \u001b[38;5;66;03m# TypeError, when given unexpected types.  So we need to catch both.\u001b[39;00m\n\u001b[1;32m    210\u001b[0m   result \u001b[38;5;241m=\u001b[39m dispatch(wrapper, args, kwargs)\n",
      "File \u001b[0;32m/data2/Anaconda3/envs/tensorflow/lib/python3.9/site-packages/tensorflow/python/ops/array_ops.py:1214\u001b[0m, in \u001b[0;36mstrided_slice\u001b[0;34m(input_, begin, end, strides, begin_mask, end_mask, ellipsis_mask, new_axis_mask, shrink_axis_mask, var, name)\u001b[0m\n\u001b[1;32m   1211\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m strides \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1212\u001b[0m   strides \u001b[38;5;241m=\u001b[39m ones_like(begin)\n\u001b[0;32m-> 1214\u001b[0m op \u001b[38;5;241m=\u001b[39m \u001b[43mgen_array_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstrided_slice\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1215\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1216\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbegin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbegin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1217\u001b[0m \u001b[43m    \u001b[49m\u001b[43mend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1218\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstrides\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstrides\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1219\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1220\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbegin_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbegin_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1221\u001b[0m \u001b[43m    \u001b[49m\u001b[43mend_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mend_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1222\u001b[0m \u001b[43m    \u001b[49m\u001b[43mellipsis_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mellipsis_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1223\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnew_axis_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_axis_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1224\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshrink_axis_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshrink_axis_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1226\u001b[0m parent_name \u001b[38;5;241m=\u001b[39m name\n\u001b[1;32m   1228\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m var \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/data2/Anaconda3/envs/tensorflow/lib/python3.9/site-packages/tensorflow/python/ops/gen_array_ops.py:10511\u001b[0m, in \u001b[0;36mstrided_slice\u001b[0;34m(input, begin, end, strides, begin_mask, end_mask, ellipsis_mask, new_axis_mask, shrink_axis_mask, name)\u001b[0m\n\u001b[1;32m  10509\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m _result\n\u001b[1;32m  10510\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m> 10511\u001b[0m   \u001b[43m_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_from_not_ok_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m  10512\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _core\u001b[38;5;241m.\u001b[39m_FallbackException:\n\u001b[1;32m  10513\u001b[0m   \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[0;32m/data2/Anaconda3/envs/tensorflow/lib/python3.9/site-packages/tensorflow/python/framework/ops.py:6941\u001b[0m, in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   6939\u001b[0m message \u001b[38;5;241m=\u001b[39m e\u001b[38;5;241m.\u001b[39mmessage \u001b[38;5;241m+\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m name: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m name \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   6940\u001b[0m \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m-> 6941\u001b[0m \u001b[43msix\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_from\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcore\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_status_to_exception\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m<string>:3\u001b[0m, in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "\u001b[0;31mInternalError\u001b[0m: Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run StridedSlice: Dst tensor is not initialized. [Op:StridedSlice] name: strided_slice/"
     ]
    }
   ],
   "source": [
    "# 初始化数据集处理器\n",
    "item_dataset_creator = NewsDataset(item_config, feature_groups['item'])\n",
    "user_dataset_creator = NewsDataset(user_config, feature_groups['user'])\n",
    "\n",
    "test_dataset = {\n",
    "    \"item\" : item_dataset_creator.create_dataset(data_path=\"/data3/zxh/news_rec/recall_csv_data/test_data/test_item_features.csv\").map(lambda x, y: x),\n",
    "    \"user\" : user_dataset_creator.create_dataset(data_path=\"/data3/zxh/news_rec/recall_csv_data/test_data/test_user_features.csv\").map(lambda x, y: x),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.2 将物料item 存入Faiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-07 15:01:52.372549: I tensorflow/stream_executor/cuda/cuda_blas.cc:1760] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"
     ]
    }
   ],
   "source": [
    "# 初始化全局索引（确保只创建一次）\n",
    "embedding_dim = None  # 会在第一个批次后确定维度\n",
    "index = None\n",
    "\n",
    "# 遍历每个批次\n",
    "for batch in test_dataset['item']:\n",
    "    # 提取当前批次的 article_id 并转换为 int64\n",
    "    article_ids = batch['article_id'].numpy()\n",
    "    article_ids_int = np.array([int(id.decode('utf-8')) for id in article_ids], dtype=np.int64)\n",
    "    \n",
    "    # 获取当前批次的嵌入向量并转换为 float32\n",
    "    output = item_embedding_model.predict(batch)\n",
    "    embeddings = np.ascontiguousarray(output.astype('float32'))\n",
    "    \n",
    "    # 如果是第一个批次，初始化索引\n",
    "    if index is None:\n",
    "        embedding_dim = embeddings.shape[1]\n",
    "        base_index = faiss.IndexFlatIP(embedding_dim)\n",
    "        index = faiss.IndexIDMap(base_index)\n",
    "    \n",
    "    # 将当前批次添加到索引\n",
    "    index.add_with_ids(embeddings, article_ids_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dssm_recall(\n",
    "    user_embedding_map: Dict[Tuple[int, int], np.ndarray],\n",
    "    user_keys: List[Tuple[int, int]],\n",
    "    faiss_index: faiss.Index,\n",
    "    k: int = 50\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    对给定的用户查询向量执行 FAISS 搜索。\n",
    "\n",
    "    Args:\n",
    "        user_embedding_map (dict): 用户嵌入字典 {(user_id, expose_hour): embedding}.\n",
    "        user_keys (List[Tuple[int, int]]): 用户分组键。\n",
    "        faiss_index (faiss.Index): FAISS 构建好的物品索引。\n",
    "        k (int): TopK 召回数量。\n",
    "\n",
    "    Returns:\n",
    "        indices (np.ndarray): shape=(num_queries, k)，每行是召回的 item 索引。\n",
    "    \"\"\"\n",
    "    if not user_keys:\n",
    "        return np.array([])\n",
    "    query_embeddings = []\n",
    "    for key in tqdm(user_keys, desc=\"Preparing user embeddings\"):\n",
    "        query_embeddings.append(user_embedding_map[key])\n",
    "\n",
    "    query_embeddings = np.stack(query_embeddings).astype(np.float32)\n",
    "    distances, indices = faiss_index.search(query_embeddings, k)\n",
    "    return indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. 多路召回合并"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.1 存储user_embedding（按照user_id和expose_hour为key）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Genrating user embbeding map: 145it [06:51,  2.84s/it]\n"
     ]
    }
   ],
   "source": [
    "user_embedding_map = {}\n",
    "\n",
    "# 遍历每个批次\n",
    "for batch in tqdm(test_dataset['user'],desc=\"Genrating user embbeding map\"):\n",
    "    user_ids = np.array([int(id.decode('utf-8')) for id in batch['user_id'].numpy()], dtype=np.int64)\n",
    "    expose_hours = batch['expose_hour'].numpy()\n",
    "\n",
    "    # 获取当前批次的用户嵌入向量（假设 output 为 np.ndarray）\n",
    "    output = user_embedding_model.predict(batch)\n",
    "    embeddings = np.ascontiguousarray(output.astype('float32'))\n",
    "\n",
    "    # 存储到 user_embedding_map 中\n",
    "    for uid, hour, emb in zip(user_ids, expose_hours, embeddings):\n",
    "        user_embedding_map[(uid, hour)] = emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (12_256_166, 4)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>user_id</th><th>article_id</th><th>expose_hour</th><th>is_clicked</th></tr><tr><td>i64</td><td>i64</td><td>i64</td><td>i64</td></tr></thead><tbody><tr><td>2413368274</td><td>465760067</td><td>4</td><td>0</td></tr><tr><td>2231512322</td><td>466772262</td><td>13</td><td>0</td></tr><tr><td>2240894400</td><td>466655257</td><td>10</td><td>0</td></tr><tr><td>1486216632</td><td>466651314</td><td>12</td><td>0</td></tr><tr><td>2439239452</td><td>466398247</td><td>0</td><td>0</td></tr><tr><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td></tr><tr><td>1406769714</td><td>466077472</td><td>13</td><td>0</td></tr><tr><td>2430514764</td><td>466449125</td><td>4</td><td>0</td></tr><tr><td>2092924226</td><td>466636411</td><td>10</td><td>0</td></tr><tr><td>1468517980</td><td>466336113</td><td>2</td><td>0</td></tr><tr><td>2440918550</td><td>466752487</td><td>13</td><td>0</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (12_256_166, 4)\n",
       "┌────────────┬────────────┬─────────────┬────────────┐\n",
       "│ user_id    ┆ article_id ┆ expose_hour ┆ is_clicked │\n",
       "│ ---        ┆ ---        ┆ ---         ┆ ---        │\n",
       "│ i64        ┆ i64        ┆ i64         ┆ i64        │\n",
       "╞════════════╪════════════╪═════════════╪════════════╡\n",
       "│ 2413368274 ┆ 465760067  ┆ 4           ┆ 0          │\n",
       "│ 2231512322 ┆ 466772262  ┆ 13          ┆ 0          │\n",
       "│ 2240894400 ┆ 466655257  ┆ 10          ┆ 0          │\n",
       "│ 1486216632 ┆ 466651314  ┆ 12          ┆ 0          │\n",
       "│ 2439239452 ┆ 466398247  ┆ 0           ┆ 0          │\n",
       "│ …          ┆ …          ┆ …           ┆ …          │\n",
       "│ 1406769714 ┆ 466077472  ┆ 13          ┆ 0          │\n",
       "│ 2430514764 ┆ 466449125  ┆ 4           ┆ 0          │\n",
       "│ 2092924226 ┆ 466636411  ┆ 10          ┆ 0          │\n",
       "│ 1468517980 ┆ 466336113  ┆ 2           ┆ 0          │\n",
       "│ 2440918550 ┆ 466752487  ┆ 13          ┆ 0          │\n",
       "└────────────┴────────────┴─────────────┴────────────┘"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.2 准备数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_user_query_data(\n",
    "    test_data: pl.DataFrame,\n",
    "    group_cols: List[str] = [\"user_id\", \"expose_hour\"]\n",
    ") -> Tuple[List[np.ndarray], List[Tuple[int, int]], List[set]]:\n",
    "    \"\"\"\n",
    "    准备用户查询向量、user_keys 和真实点击集合。\n",
    "    \"\"\"\n",
    "\n",
    "    # 聚合曝光集 & 点击集\n",
    "    grouped = test_data.group_by(group_cols).agg([\n",
    "        pl.col(\"article_id\").unique().alias(\"expose_set\"),\n",
    "        pl.col(\"article_id\").filter(pl.col(\"is_clicked\") == 1).unique().alias(\"click_set\")\n",
    "    ])\n",
    "\n",
    "    # 提取 user_keys 和 sets\n",
    "    user_keys = list(zip(\n",
    "        grouped[\"user_id\"].to_numpy(),\n",
    "        grouped[\"expose_hour\"].to_list()\n",
    "    ))\n",
    "\n",
    "    expose_lists = [set(x) for x in grouped[\"expose_set\"].to_list()]\n",
    "    click_lists = [set(x) for x in grouped[\"click_set\"].to_list()]\n",
    "\n",
    "    return user_keys, expose_lists, click_lists\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_keys, expose_lists, click_lists = prepare_user_query_data(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.3 召回合并，测试Recall 和 Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preparing user embeddings:   3%|▎         | 31335/910176 [00:00<00:02, 313313.70it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preparing user embeddings: 100%|██████████| 910176/910176 [00:02<00:00, 370272.84it/s]\n"
     ]
    }
   ],
   "source": [
    "dssm_recall_lists = dssm_recall(user_embedding_map, user_keys, index, k=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Genearting itemcf & swing recall results:   1%|          | 10181/910176 [00:27<34:03, 440.31it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Genearting itemcf & swing recall results:  38%|███▊      | 341758/910176 [15:38<24:46, 382.34it/s]  "
     ]
    }
   ],
   "source": [
    "itemcf_recall_lists = []\n",
    "swing_recall_lists = []\n",
    "for user_id, expose_hour in tqdm(user_keys, desc=\"Genearting itemcf & swing recall results\"):\n",
    "    itemcf_recall_lists.append(itemcf_recall(int(user_id), user_item_time_dict, itemcf_inverted_index))\n",
    "    swing_recall_lists.append(swing_recall(int(user_id), user_item_time_dict, swing_inverted_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_recall(\n",
    "    recall_results: Dict[str, List[List[int]]],\n",
    "    click_lists: List[Set[int]],\n",
    "    expose_lists: List[Set[int]],\n",
    "    recall_k_config: Dict[str, int]\n",
    ") -> Tuple[float, float, float]:\n",
    "    \"\"\"\n",
    "    多路召回融合后计算 Precision@K、Recall@K 和 HitRate@K。\n",
    "\n",
    "    Args:\n",
    "        recall_results (dict): 每条召回通道的召回结果。\n",
    "        click_lists (List[set]): 每个用户的真实点击集合。\n",
    "        expose_lists (List[set]): 每个用户的真实曝光集合。\n",
    "        recall_k_config (dict): 每条召回路径保留的 Top-K 数量，如 {'itemcf': 20, 'swing': 10, 'dssm': 30}。\n",
    "\n",
    "    Returns:\n",
    "        Tuple[float, float, float]: 合并后的 precision@k、recall@k 和 hitrate@k。\n",
    "    \"\"\"\n",
    "    total_precision = 0.0\n",
    "    total_recall = 0.0\n",
    "    total_hitrate = 0.0\n",
    "    total_requests = 0\n",
    "\n",
    "    for i in tqdm(range(len(click_lists)), desc=\"Evaluating Merged Recall\"):\n",
    "        merged_predicted_ids = set()\n",
    "\n",
    "        for method, recall_list in recall_results.items():\n",
    "            topk = recall_k_config.get(method, 0)\n",
    "            if i < len(recall_list):\n",
    "                merged_predicted_ids.update(recall_list[i][:topk])\n",
    "\n",
    "        actual_article_ids = click_lists[i]\n",
    "        group_size = len(expose_lists[i])\n",
    "\n",
    "        if not actual_article_ids or not merged_predicted_ids or group_size == 0:\n",
    "            continue\n",
    "\n",
    "        num_correct = len(merged_predicted_ids & actual_article_ids)\n",
    "        precision_k = num_correct / len(merged_predicted_ids)\n",
    "        recall_k = num_correct / len(actual_article_ids)\n",
    "        hitrate_k = num_correct\n",
    "\n",
    "        total_precision += precision_k * group_size\n",
    "        total_recall += recall_k * group_size\n",
    "        total_hitrate += hitrate_k\n",
    "        total_requests += group_size\n",
    "\n",
    "    final_precision = total_precision / total_requests if total_requests else 0.0\n",
    "    final_recall = total_recall / total_requests if total_requests else 0.0\n",
    "    final_hitrate = total_hitrate / sum(len(s) for s in click_lists) if click_lists else 0.0\n",
    "\n",
    "    return final_precision, final_recall, final_hitrate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Merged Recall:   0%|          | 0/910176 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Merged Recall: 100%|██████████| 910176/910176 [00:07<00:00, 114151.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged Recall -> Precision @ 50:       0.0068, Recall @ 50: 0.0877,       HitRate @ 50: 0.1110\n"
     ]
    }
   ],
   "source": [
    "recall_results = {\n",
    "    #\"itemcf\": itemcf_recall_lists,\n",
    "    # \"swing\": swing_recall_lists,\n",
    "    \"dssm\": dssm_recall_lists\n",
    "}\n",
    "\n",
    "recall_k_config = {\n",
    "    # \"itemcf\": 0,\n",
    "    # \"swing\": 0,\n",
    "    \"dssm\": 50\n",
    "}\n",
    "\n",
    "precision, recall, hr = evaluate_recall(\n",
    "    recall_results=recall_results,\n",
    "    click_lists=click_lists,\n",
    "    expose_lists=expose_lists,\n",
    "    recall_k_config=recall_k_config\n",
    ")\n",
    "\n",
    "print(f\"Merged Recall -> Precision @ {sum(recall_k_config.values())}: \\\n",
    "      {precision:.4f}, Recall @ {sum(recall_k_config.values())}: {recall:.4f}, \\\n",
    "      HitRate @ {sum(recall_k_config.values())}: {hr:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
